{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb9b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating temporal usage patterns...\n",
      "Generated 98 sessions over 30 days\n",
      "Generating engagement depth indicators...\n",
      "Generating content consumption patterns...\n",
      "Generating behavioral triggers...\n",
      "Generating wellbeing indicators...\n",
      "Consolidating data...\n",
      "Consolidated dataset created with 98 sessions and 28 columns\n",
      "Saved consolidated data with 98 sessions\n",
      "\n",
      "Dataset shape: (98, 28)\n",
      "Sessions per day range: 3 - 4\n",
      "Average sessions per day: 3.27\n",
      "\n",
      "Column names:\n",
      "['session_id', 'day', 'session_start_hour', 'session_duration_minutes', 'time_since_last_session_hours', 'is_weekend', 'user_type', 'scroll_velocity_posts_per_min', 'scroll_depth_percentage', 'interaction_rate_percentage', 'time_per_post_seconds', 'return_to_feed_frequency', 'primary_content_type', 'preferred_categories', 'negative_content_minutes', 'algorithm_interactions', 'search_vs_feed_ratio', 'notification_response_minutes', 'opening_trigger', 'background_usage_score', 'exit_behavior', 'planned_usage_minutes', 'actual_usage_minutes', 'usage_overage_minutes', 'late_night_usage_minutes', 'compulsive_checks_count', 'sleep_hours', 'baseline_compulsiveness']\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def generate_consolidated_social_media_dataset(days=30):\n",
    "    \"\"\"Generate consolidated social media usage dataset with consistent sessions\"\"\"\n",
    "    \n",
    "    # Step 1: Generate temporal usage patterns first to establish sessions\n",
    "    print(\"Generating temporal usage patterns...\")\n",
    "    temporal_data = []\n",
    "    \n",
    "    # User-specific patterns\n",
    "    base_sessions_per_day = np.random.gamma(2, 3)  # 2-20 sessions/day\n",
    "    weekend_multiplier = np.random.uniform(1.2, 2.0)  # More usage on weekends\n",
    "    \n",
    "    session_id = 0\n",
    "    for day in range(days):\n",
    "        is_weekend = day % 7 in [5, 6]\n",
    "        sessions_today = int(base_sessions_per_day * (weekend_multiplier if is_weekend else 1))\n",
    "        \n",
    "        # Generate sessions for this day\n",
    "        session_times = np.random.uniform(0, 24, sessions_today)\n",
    "        \n",
    "        for session_time in session_times:\n",
    "            # Session duration (log-normal distribution, 1-60 minutes)\n",
    "            duration = np.random.lognormal(2.5, 1.2)  \n",
    "            duration = np.clip(duration, 1, 120)  # 1-120 minutes\n",
    "            \n",
    "            # Time between sessions\n",
    "            time_since_last = np.random.exponential(2)  # Hours\n",
    "            \n",
    "            # Peak usage hours (evening bias)\n",
    "            if 18 <= session_time <= 23:\n",
    "                duration *= np.random.uniform(1.5, 2.5)\n",
    "            \n",
    "            temporal_data.append({\n",
    "                'session_id': session_id,\n",
    "                'day': day,\n",
    "                'session_start_hour': session_time,\n",
    "                'session_duration_minutes': duration,\n",
    "                'time_since_last_session_hours': time_since_last,\n",
    "                'is_weekend': is_weekend\n",
    "            })\n",
    "            session_id += 1\n",
    "    \n",
    "    temporal_df = pd.DataFrame(temporal_data)\n",
    "    total_sessions = len(temporal_df)\n",
    "    print(f\"Generated {total_sessions} sessions over {days} days\")\n",
    "    \n",
    "    # Step 2: Generate engagement depth indicators for each session\n",
    "    print(\"Generating engagement depth indicators...\")\n",
    "    engagement_data = []\n",
    "    \n",
    "    for session_id in range(total_sessions):\n",
    "        # User type influences behavior\n",
    "        user_type = np.random.choice(['casual', 'moderate', 'heavy'], p=[0.4, 0.4, 0.2])\n",
    "        \n",
    "        # Scroll velocity (posts per minute)\n",
    "        if user_type == 'casual':\n",
    "            scroll_velocity = np.random.gamma(2, 2)  # Slower scrolling\n",
    "        elif user_type == 'moderate':\n",
    "            scroll_velocity = np.random.gamma(3, 3)\n",
    "        else:  # heavy user\n",
    "            scroll_velocity = np.random.gamma(5, 4)  # Faster scrolling\n",
    "            \n",
    "        # Scroll depth (relative to feed length)\n",
    "        scroll_depth = np.random.beta(2, 3)  # Most users don't scroll to bottom\n",
    "        \n",
    "        # Content interaction rate (interactions per 100 posts viewed)\n",
    "        base_interaction_rate = np.random.beta(2, 8) * 20  # 0-20%\n",
    "        \n",
    "        # Time spent per post (seconds)\n",
    "        time_per_post = np.random.lognormal(1.5, 0.8)\n",
    "        time_per_post = np.clip(time_per_post, 1, 60)\n",
    "        \n",
    "        # Return-to-feed frequency (times per session)\n",
    "        return_frequency = np.random.poisson(3)\n",
    "        \n",
    "        engagement_data.append({\n",
    "            'session_id': session_id,\n",
    "            'user_type': user_type,\n",
    "            'scroll_velocity_posts_per_min': scroll_velocity,\n",
    "            'scroll_depth_percentage': scroll_depth * 100,\n",
    "            'interaction_rate_percentage': base_interaction_rate,\n",
    "            'time_per_post_seconds': time_per_post,\n",
    "            'return_to_feed_frequency': return_frequency\n",
    "        })\n",
    "    \n",
    "    engagement_df = pd.DataFrame(engagement_data)\n",
    "    \n",
    "    # Step 3: Generate content consumption patterns for each session\n",
    "    print(\"Generating content consumption patterns...\") \n",
    "    content_data = []\n",
    "    \n",
    "    content_types = ['photo', 'video', 'story', 'reel', 'text']\n",
    "    content_categories = ['lifestyle', 'news', 'entertainment', 'sports', 'politics', 'wellness']\n",
    "    \n",
    "    for session_id in range(total_sessions):\n",
    "        # User preferences\n",
    "        preferred_type = np.random.choice(content_types)\n",
    "        preferred_categories = np.random.choice(content_categories, size=2, replace=False)\n",
    "        \n",
    "        # Content type distribution\n",
    "        type_probs = [0.2] * len(content_types)\n",
    "        type_probs[content_types.index(preferred_type)] = 0.4\n",
    "        type_consumed = np.random.choice(content_types, p=np.array(type_probs)/sum(type_probs))\n",
    "        \n",
    "        # Time on negative/controversial content\n",
    "        negative_content_time = np.random.exponential(5)  # Minutes\n",
    "        \n",
    "        # Recommendation algorithm engagement\n",
    "        algo_clicks = np.random.poisson(8)\n",
    "        \n",
    "        # Search vs feed browsing ratio\n",
    "        search_ratio = np.random.beta(1, 4)  # Most time in feed, some searching\n",
    "        \n",
    "        content_data.append({\n",
    "            'session_id': session_id,\n",
    "            'primary_content_type': type_consumed,\n",
    "            'preferred_categories': ','.join(preferred_categories),\n",
    "            'negative_content_minutes': negative_content_time,\n",
    "            'algorithm_interactions': algo_clicks,\n",
    "            'search_vs_feed_ratio': search_ratio\n",
    "        })\n",
    "    \n",
    "    content_df = pd.DataFrame(content_data)\n",
    "    \n",
    "    # Step 4: Generate behavioral triggers for each session\n",
    "    print(\"Generating behavioral triggers...\")\n",
    "    trigger_data = []\n",
    "    \n",
    "    trigger_types = ['notification', 'boredom', 'habit', 'social', 'news_check']\n",
    "    \n",
    "    for session_id in range(total_sessions):\n",
    "        # Notification response pattern\n",
    "        notification_delay = np.random.lognormal(1, 1.5)  # Minutes to respond\n",
    "        notification_delay = np.clip(notification_delay, 0.1, 120)\n",
    "        \n",
    "        # Trigger for opening app\n",
    "        opening_trigger = np.random.choice(trigger_types, \n",
    "                                         p=[0.3, 0.25, 0.2, 0.15, 0.1])\n",
    "        \n",
    "        # Background app usage (0-1 scale)\n",
    "        background_usage = np.random.beta(2, 5)\n",
    "        \n",
    "        # How session ended\n",
    "        exit_type = np.random.choice(['intentional', 'distracted', 'forced'], \n",
    "                                   p=[0.4, 0.4, 0.2])\n",
    "        \n",
    "        trigger_data.append({\n",
    "            'session_id': session_id,\n",
    "            'notification_response_minutes': notification_delay,\n",
    "            'opening_trigger': opening_trigger,\n",
    "            'background_usage_score': background_usage,\n",
    "            'exit_behavior': exit_type\n",
    "        })\n",
    "    \n",
    "    trigger_df = pd.DataFrame(trigger_data)\n",
    "    \n",
    "    # Step 5: Generate wellbeing indicators (daily level)\n",
    "    print(\"Generating wellbeing indicators...\")\n",
    "    wellbeing_data = []\n",
    "    \n",
    "    # User baseline characteristics\n",
    "    baseline_compulsiveness = np.random.beta(2, 5)  # Most users low, some high\n",
    "    baseline_sleep_quality = np.random.normal(7, 1.5)  # Hours of sleep\n",
    "    \n",
    "    for day in range(days):\n",
    "        # Usage exceeding planned time\n",
    "        planned_time = np.random.uniform(30, 120)  # Minutes planned\n",
    "        actual_time = planned_time * np.random.lognormal(0, 0.5)  # Usually more\n",
    "        time_overage = max(0, actual_time - planned_time)\n",
    "        \n",
    "        # Late night usage\n",
    "        late_night_minutes = np.random.exponential(15) if np.random.random() < 0.3 else 0\n",
    "        \n",
    "        # Compulsive checking\n",
    "        daily_compulsive_checks = np.random.poisson(baseline_compulsiveness * 20)\n",
    "        \n",
    "        # Sleep impact\n",
    "        sleep_hours = baseline_sleep_quality - (late_night_minutes / 60) * 0.5\n",
    "        sleep_hours = np.clip(sleep_hours, 4, 10)\n",
    "        \n",
    "        wellbeing_data.append({\n",
    "            'day': day,\n",
    "            'planned_usage_minutes': planned_time,\n",
    "            'actual_usage_minutes': actual_time,\n",
    "            'usage_overage_minutes': time_overage,\n",
    "            'late_night_usage_minutes': late_night_minutes,\n",
    "            'compulsive_checks_count': daily_compulsive_checks,\n",
    "            'sleep_hours': sleep_hours,\n",
    "            'baseline_compulsiveness': baseline_compulsiveness\n",
    "        })\n",
    "    \n",
    "    wellbeing_df = pd.DataFrame(wellbeing_data)\n",
    "    \n",
    "    # Step 6: Merge all DataFrames into one consolidated DataFrame\n",
    "    print(\"Consolidating data...\")\n",
    "    \n",
    "    # Start with temporal data as the base\n",
    "    consolidated_df = temporal_df\n",
    "    \n",
    "    # Merge engagement data\n",
    "    consolidated_df = consolidated_df.merge(engagement_df, on='session_id', how='left')\n",
    "    \n",
    "    # Merge content data\n",
    "    consolidated_df = consolidated_df.merge(content_df, on='session_id', how='left')\n",
    "    \n",
    "    # Merge behavioral trigger data\n",
    "    consolidated_df = consolidated_df.merge(trigger_df, on='session_id', how='left')\n",
    "    \n",
    "    # Merge wellbeing data (daily level, so merge on 'day')\n",
    "    consolidated_df = consolidated_df.merge(wellbeing_df, on='day', how='left')\n",
    "    \n",
    "    print(f\"Consolidated dataset created with {len(consolidated_df)} sessions and {len(consolidated_df.columns)} columns\")\n",
    "    \n",
    "    return consolidated_df\n",
    "\n",
    "# Generate the consolidated dataset\n",
    "consolidated_data = generate_consolidated_social_media_dataset(days=30)\n",
    "\n",
    "# Save to CSV\n",
    "consolidated_data.to_csv('synthetic_social_media_data_consolidated.csv', index=False)\n",
    "print(f\"Saved consolidated data with {len(consolidated_data)} sessions\")\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"\\nDataset shape: {consolidated_data.shape}\")\n",
    "print(f\"Sessions per day range: {consolidated_data.groupby('day').size().min()} - {consolidated_data.groupby('day').size().max()}\")\n",
    "print(f\"Average sessions per day: {consolidated_data.groupby('day').size().mean():.2f}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(consolidated_data.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
