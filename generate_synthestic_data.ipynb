{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb9b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating temporal usage patterns...\n",
      "Generated 250 sessions over 30 days\n",
      "Generating engagement depth indicators...\n",
      "Generating rich content consumption patterns for LLM summaries...\n",
      "Generating behavioral triggers...\n",
      "Generating wellbeing indicators...\n",
      "Consolidating data...\n",
      "Consolidated dataset created with 250 sessions and 54 columns\n",
      "Saved consolidated data with 250 sessions\n",
      "\n",
      "Dataset shape: (250, 54)\n",
      "Sessions per day range: 7 - 12\n",
      "Average sessions per day: 8.33\n",
      "\n",
      "Column names:\n",
      "['session_id', 'day', 'session_start_hour_decimal', 'session_start_time', 'session_duration_minutes', 'user_type', 'scroll_velocity_posts_per_min', 'interaction_rate_percentage', 'time_per_post_seconds', 'return_to_feed_frequency', 'primary_content_type', 'preferred_categories', 'negative_content_minutes', 'algorithm_interactions', 'search_vs_feed_ratio', 'went_down_rabbit_hole', 'rabbit_hole_topic', 'rabbit_hole_duration_minutes', 'had_cringe_moment', 'cringe_description', 'binge_watched_content', 'binge_topic', 'binge_count', 'was_procrastinating', 'procrastination_topic', 'procrastination_irony_level', 'social_stalking', 'stalking_behavior', 'emotional_state', 'mood_influenced_content', 'ads_seen_count', 'shopping_ads_clicked', 'added_to_cart_no_buy', 'attempted_to_post', 'deleted_before_posting', 'selfies_taken_count', 'compared_life_to_others', 'felt_fomo', 'lifestyle_envy_minutes', 'watched_food_while_hungry', 'vacation_photos_in_winter', 'random_research', 'random_research_topic', 'notification_response_minutes', 'opening_trigger', 'background_usage_score', 'exit_behavior', 'planned_usage_minutes', 'actual_usage_minutes', 'usage_overage_minutes', 'late_night_usage_minutes', 'compulsive_checks_count', 'sleep_hours', 'baseline_compulsiveness']\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducible output\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def generate_consolidated_social_media_dataset(days=30):\n",
    "    \"\"\"Generate consolidated social media usage dataset with consistent sessions\"\"\"\n",
    "    \n",
    "    # Step 1: Generate temporal usage patterns first to establish sessions\n",
    "    print(\"Generating temporal usage patterns...\")\n",
    "    temporal_data = []\n",
    "    \n",
    "    # User-specific patterns\n",
    "    base_sessions_per_day = np.random.gamma(2, 3)  # 2-20 sessions/day\n",
    "    weekend_multiplier = np.random.uniform(1.2, 2.0)  # More usage on weekends\n",
    "    \n",
    "    session_id = 0\n",
    "    for day in range(days):\n",
    "        is_weekend = day % 7 in [5, 6]\n",
    "        sessions_today = int(base_sessions_per_day * (weekend_multiplier if is_weekend else 1))\n",
    "        \n",
    "        # Generate sessions for this day\n",
    "        session_times = np.random.uniform(0, 24, sessions_today)\n",
    "        \n",
    "        for session_time in session_times:\n",
    "            # Session duration (log-normal distribution, 1-60 minutes)\n",
    "            duration = np.random.lognormal(2.5, 1.2)  \n",
    "            duration = np.clip(duration, 1, 120)  # 1-120 minutes\n",
    "            \n",
    "            # Time between sessions\n",
    "            time_since_last = np.random.exponential(2)  # Hours\n",
    "            \n",
    "            # Peak usage hours (evening bias)\n",
    "            if 18 <= session_time <= 23:\n",
    "                duration *= np.random.uniform(1.5, 2.5)\n",
    "            \n",
    "            # Convert decimal hour to readable time format\n",
    "            hour = int(session_time)\n",
    "            minute = int((session_time - hour) * 60)\n",
    "            session_time_readable = f\"{hour:02d}:{minute:02d}\"\n",
    "            \n",
    "            temporal_data.append({\n",
    "                'session_id': session_id,\n",
    "                'day': day,\n",
    "                'session_start_hour_decimal': session_time,\n",
    "                'session_start_time': session_time_readable,\n",
    "                'session_duration_minutes': duration,\n",
    "                #'time_since_last_session_hours': time_since_last,\n",
    "                #'is_weekend': is_weekend\n",
    "            })\n",
    "            session_id += 1\n",
    "    \n",
    "    temporal_df = pd.DataFrame(temporal_data)\n",
    "    total_sessions = len(temporal_df)\n",
    "    print(f\"Generated {total_sessions} sessions over {days} days\")\n",
    "    \n",
    "    # Step 2: Generate engagement depth indicators for each session\n",
    "    print(\"Generating engagement depth indicators...\")\n",
    "    engagement_data = []\n",
    "    \n",
    "    for session_id in range(total_sessions):\n",
    "        # User type influences behavior\n",
    "        user_type = np.random.choice(['casual', 'moderate', 'heavy'], p=[0.4, 0.4, 0.2])\n",
    "        \n",
    "        # Scroll velocity (posts per minute)\n",
    "        if user_type == 'casual':\n",
    "            scroll_velocity = np.random.gamma(2, 2)  # Slower scrolling\n",
    "        elif user_type == 'moderate':\n",
    "            scroll_velocity = np.random.gamma(3, 3)\n",
    "        else:  # heavy user\n",
    "            scroll_velocity = np.random.gamma(5, 4)  # Faster scrolling\n",
    "            \n",
    "        # Scroll depth (relative to feed length)\n",
    "        scroll_depth = np.random.beta(2, 3)  # Most users don't scroll to bottom\n",
    "        \n",
    "        # Content interaction rate (interactions per 100 posts viewed)\n",
    "        base_interaction_rate = np.random.beta(2, 8) * 20  # 0-20%\n",
    "        \n",
    "        # Time spent per post (seconds)\n",
    "        time_per_post = np.random.lognormal(1.5, 0.8)\n",
    "        time_per_post = np.clip(time_per_post, 1, 60)\n",
    "        \n",
    "        # Return-to-feed frequency (times per session)\n",
    "        return_frequency = np.random.poisson(3)\n",
    "        \n",
    "        engagement_data.append({\n",
    "            'session_id': session_id,\n",
    "            'user_type': user_type,\n",
    "            'scroll_velocity_posts_per_min': scroll_velocity,\n",
    "            #'scroll_depth_percentage': scroll_depth * 100,\n",
    "            'interaction_rate_percentage': base_interaction_rate,\n",
    "            'time_per_post_seconds': time_per_post,\n",
    "            'return_to_feed_frequency': return_frequency\n",
    "        })\n",
    "    \n",
    "    engagement_df = pd.DataFrame(engagement_data)\n",
    "    \n",
    "    # Step 3: Generate RICH content consumption patterns for each session\n",
    "    print(\"Generating rich content consumption patterns for LLM summaries...\") \n",
    "    content_data = []\n",
    "    \n",
    "    content_types = ['photo', 'video', 'story', 'reel', 'text']\n",
    "    content_categories = ['lifestyle', 'news', 'entertainment', 'sports', 'politics', 'wellness', \n",
    "                         'food', 'travel', 'fashion', 'tech', 'memes', 'pets']\n",
    "    \n",
    "    # Quirky content behaviors for funny summaries\n",
    "    weird_rabbit_holes = ['conspiracy theories about birds', 'DIY furniture fails', 'medieval recipes',\n",
    "                         'people arguing about pineapple on pizza', 'cats wearing tiny hats',\n",
    "                         'TikTok dances from 2019', 'arguing about which cereal is best',\n",
    "                         'reviews of gas station bathrooms', 'people organizing their sock drawers',\n",
    "                         'weird food combinations', 'failed craft projects', 'relationship advice from teenagers']\n",
    "    \n",
    "    cringe_moments = ['accidentally liked ex\\'s photo from 2018', 'watched own old stories', \n",
    "                     'replied to wrong person', 'posted and immediately deleted',\n",
    "                     'argued with a bot', 'fell for clickbait about celebrities',\n",
    "                     'spent 20 mins on someone\\'s vacation photos', 'stalked college acquaintance']\n",
    "    \n",
    "    binge_behaviors = ['watched every cat video in existence', 'read 47 relationship advice posts',\n",
    "                      'looked at food for 30 minutes while hungry', 'watched house tours of strangers',\n",
    "                      'binged workout videos without working out', 'researched products to never buy',\n",
    "                      'watched makeup tutorials with no intention of doing makeup']\n",
    "    \n",
    "    procrastination_content = ['productivity tips while procrastinating', 'study motivation videos',\n",
    "                              'organization hacks for messy room', 'workout videos from bed',\n",
    "                              'healthy recipes while eating junk food', 'financial advice while shopping']\n",
    "    \n",
    "    social_behaviors = ['lurked on high school friend\\'s profile', 'checked if crush viewed story',\n",
    "                       'analyzed response time patterns', 'compared life to influencers',\n",
    "                       'screenshot funny posts for group chat', 'planned perfect response for 10 minutes']\n",
    "    \n",
    "    for session_id in range(total_sessions):\n",
    "        # User preferences\n",
    "        preferred_type = np.random.choice(content_types)\n",
    "        preferred_categories = np.random.choice(content_categories, size=np.random.randint(2, 4), replace=False)\n",
    "        \n",
    "        # Content type distribution\n",
    "        type_probs = [0.15] * len(content_types)\n",
    "        type_probs[content_types.index(preferred_type)] = 0.4\n",
    "        type_consumed = np.random.choice(content_types, p=np.array(type_probs)/sum(type_probs))\n",
    "        \n",
    "        # Time on negative/controversial content\n",
    "        negative_content_time = np.random.exponential(5)  # Minutes\n",
    "        \n",
    "        # Recommendation algorithm engagement\n",
    "        algo_clicks = np.random.poisson(8)\n",
    "        \n",
    "        # Search vs feed browsing ratio\n",
    "        search_ratio = np.random.beta(1, 4)  # Most time in feed, some searching\n",
    "        \n",
    "        # NEW RICH CONTENT PATTERNS FOR FUNNY SUMMARIES\n",
    "        \n",
    "        # Rabbit hole adventures\n",
    "        went_down_rabbit_hole = np.random.random() < 0.3\n",
    "        rabbit_hole_topic = np.random.choice(weird_rabbit_holes) if went_down_rabbit_hole else None\n",
    "        rabbit_hole_duration = np.random.exponential(15) if went_down_rabbit_hole else 0\n",
    "        \n",
    "        # Cringe moments\n",
    "        had_cringe_moment = np.random.random() < 0.2\n",
    "        cringe_description = np.random.choice(cringe_moments) if had_cringe_moment else None\n",
    "        \n",
    "        # Binge watching behavior\n",
    "        binge_watched = np.random.random() < 0.4\n",
    "        binge_topic = np.random.choice(binge_behaviors) if binge_watched else None\n",
    "        binge_count = np.random.poisson(12) if binge_watched else 0\n",
    "        \n",
    "        # Procrastination patterns\n",
    "        procrastinating = np.random.random() < 0.35\n",
    "        procrastination_topic = np.random.choice(procrastination_content) if procrastinating else None\n",
    "        procrastination_irony_level = np.random.uniform(0, 10) if procrastinating else 0\n",
    "        \n",
    "        # Social stalking behavior\n",
    "        social_stalking = np.random.random() < 0.25\n",
    "        stalking_behavior = np.random.choice(social_behaviors) if social_stalking else None\n",
    "        \n",
    "        # Emotional state influence on content\n",
    "        emotional_state = np.random.choice(['happy', 'sad', 'bored', 'anxious', 'excited', 'stressed'])\n",
    "        mood_influenced_content = np.random.random() < 0.6\n",
    "        \n",
    "        # Shopping temptation\n",
    "        saw_ads_count = np.random.poisson(5)\n",
    "        clicked_shopping_ads = np.random.poisson(2)\n",
    "        added_to_cart_but_didnt_buy = np.random.random() < 0.4\n",
    "        \n",
    "        # Content creation attempts\n",
    "        tried_to_post = np.random.random() < 0.3\n",
    "        deleted_before_posting = np.random.random() < 0.7 if tried_to_post else False\n",
    "        took_selfies_count = np.random.poisson(3) if tried_to_post else 0\n",
    "        \n",
    "        # FOMO and comparison behavior\n",
    "        compared_life_to_others = np.random.random() < 0.4\n",
    "        felt_fomo = np.random.random() < 0.3\n",
    "        lifestyle_envy_minutes = np.random.exponential(8) if compared_life_to_others else 0\n",
    "        \n",
    "        # Random specific behaviors for humor\n",
    "        watched_food_while_hungry = np.random.random() < 0.2\n",
    "        looked_at_vacation_photos_in_winter = np.random.random() < 0.15\n",
    "        researched_random_topic = np.random.random() < 0.25\n",
    "        random_research_topic = np.random.choice(['how to become a beekeeper', 'cost of living in Iceland',\n",
    "                                                'can cats be vegetarian', 'how to build a treehouse',\n",
    "                                                'are dolphins really that smart', 'why do we say OK'])\n",
    "        \n",
    "        content_data.append({\n",
    "            'session_id': session_id,\n",
    "            'primary_content_type': type_consumed,\n",
    "            'preferred_categories': ','.join(preferred_categories),\n",
    "            'negative_content_minutes': negative_content_time,\n",
    "            'algorithm_interactions': algo_clicks,\n",
    "            'search_vs_feed_ratio': search_ratio,\n",
    "            \n",
    "            # Rich behavioral data for funny summaries\n",
    "            'went_down_rabbit_hole': went_down_rabbit_hole,\n",
    "            'rabbit_hole_topic': rabbit_hole_topic,\n",
    "            'rabbit_hole_duration_minutes': rabbit_hole_duration,\n",
    "            'had_cringe_moment': had_cringe_moment,\n",
    "            'cringe_description': cringe_description,\n",
    "            'binge_watched_content': binge_watched,\n",
    "            'binge_topic': binge_topic,\n",
    "            'binge_count': binge_count,\n",
    "            'was_procrastinating': procrastinating,\n",
    "            'procrastination_topic': procrastination_topic,\n",
    "            'procrastination_irony_level': procrastination_irony_level,\n",
    "            'social_stalking': social_stalking,\n",
    "            'stalking_behavior': stalking_behavior,\n",
    "            'emotional_state': emotional_state,\n",
    "            'mood_influenced_content': mood_influenced_content,\n",
    "            'ads_seen_count': saw_ads_count,\n",
    "            'shopping_ads_clicked': clicked_shopping_ads,\n",
    "            'added_to_cart_no_buy': added_to_cart_but_didnt_buy,\n",
    "            'attempted_to_post': tried_to_post,\n",
    "            'deleted_before_posting': deleted_before_posting,\n",
    "            'selfies_taken_count': took_selfies_count,\n",
    "            'compared_life_to_others': compared_life_to_others,\n",
    "            'felt_fomo': felt_fomo,\n",
    "            'lifestyle_envy_minutes': lifestyle_envy_minutes,\n",
    "            'watched_food_while_hungry': watched_food_while_hungry,\n",
    "            'vacation_photos_in_winter': looked_at_vacation_photos_in_winter,\n",
    "            'random_research': researched_random_topic,\n",
    "            'random_research_topic': random_research_topic if researched_random_topic else None\n",
    "        })\n",
    "    \n",
    "    content_df = pd.DataFrame(content_data)\n",
    "    \n",
    "    # Step 4: Generate behavioral triggers for each session\n",
    "    print(\"Generating behavioral triggers...\")\n",
    "    trigger_data = []\n",
    "    \n",
    "    trigger_types = ['notification', 'boredom', 'habit', 'social', 'news_check']\n",
    "    \n",
    "    for session_id in range(total_sessions):\n",
    "        # Notification response pattern\n",
    "        notification_delay = np.random.lognormal(1, 1.5)  # Minutes to respond\n",
    "        notification_delay = np.clip(notification_delay, 0.1, 120)\n",
    "        \n",
    "        # Trigger for opening app\n",
    "        opening_trigger = np.random.choice(trigger_types, \n",
    "                                         p=[0.3, 0.25, 0.2, 0.15, 0.1])\n",
    "        \n",
    "        # Background app usage (0-1 scale)\n",
    "        background_usage = np.random.beta(2, 5)\n",
    "        \n",
    "        # How session ended\n",
    "        exit_type = np.random.choice(['intentional', 'distracted', 'forced'], \n",
    "                                   p=[0.4, 0.4, 0.2])\n",
    "        \n",
    "        trigger_data.append({\n",
    "            'session_id': session_id,\n",
    "            'notification_response_minutes': notification_delay,\n",
    "            'opening_trigger': opening_trigger,\n",
    "            'background_usage_score': background_usage,\n",
    "            'exit_behavior': exit_type\n",
    "        })\n",
    "    \n",
    "    trigger_df = pd.DataFrame(trigger_data)\n",
    "    \n",
    "    # Step 5: Generate wellbeing indicators (daily level)\n",
    "    print(\"Generating wellbeing indicators...\")\n",
    "    wellbeing_data = []\n",
    "    \n",
    "    # User baseline characteristics\n",
    "    baseline_compulsiveness = np.random.beta(2, 5)  # Most users low, some high\n",
    "    baseline_sleep_quality = np.random.normal(7, 1.5)  # Hours of sleep\n",
    "    \n",
    "    for day in range(days):\n",
    "        # Usage exceeding planned time\n",
    "        planned_time = np.random.uniform(30, 120)  # Minutes planned\n",
    "        actual_time = planned_time * np.random.lognormal(0, 0.5)  # Usually more\n",
    "        time_overage = max(0, actual_time - planned_time)\n",
    "        \n",
    "        # Late night usage\n",
    "        late_night_minutes = np.random.exponential(15) if np.random.random() < 0.3 else 0\n",
    "        \n",
    "        # Compulsive checking\n",
    "        daily_compulsive_checks = np.random.poisson(baseline_compulsiveness * 20)\n",
    "        \n",
    "        # Sleep impact\n",
    "        sleep_hours = baseline_sleep_quality - (late_night_minutes / 60) * 0.5\n",
    "        sleep_hours = np.clip(sleep_hours, 4, 10)\n",
    "        \n",
    "        wellbeing_data.append({\n",
    "            'day': day,\n",
    "            'planned_usage_minutes': planned_time,\n",
    "            'actual_usage_minutes': actual_time,\n",
    "            'usage_overage_minutes': time_overage,\n",
    "            'late_night_usage_minutes': late_night_minutes,\n",
    "            'compulsive_checks_count': daily_compulsive_checks,\n",
    "            'sleep_hours': sleep_hours,\n",
    "            'baseline_compulsiveness': baseline_compulsiveness\n",
    "        })\n",
    "    \n",
    "    wellbeing_df = pd.DataFrame(wellbeing_data)\n",
    "    \n",
    "    # Step 6: Merge all DataFrames into one consolidated DataFrame\n",
    "    print(\"Consolidating data...\")\n",
    "    \n",
    "    # Start with temporal data as the base\n",
    "    consolidated_df = temporal_df\n",
    "    \n",
    "    # Merge engagement data\n",
    "    consolidated_df = consolidated_df.merge(engagement_df, on='session_id', how='left')\n",
    "    \n",
    "    # Merge content data\n",
    "    consolidated_df = consolidated_df.merge(content_df, on='session_id', how='left')\n",
    "    \n",
    "    # Merge behavioral trigger data\n",
    "    consolidated_df = consolidated_df.merge(trigger_df, on='session_id', how='left')\n",
    "    \n",
    "    # Merge wellbeing data (daily level, so merge on 'day')\n",
    "    consolidated_df = consolidated_df.merge(wellbeing_df, on='day', how='left')\n",
    "    \n",
    "    print(f\"Consolidated dataset created with {len(consolidated_df)} sessions and {len(consolidated_df.columns)} columns\")\n",
    "    \n",
    "    return consolidated_df\n",
    "\n",
    "# Generate the consolidated dataset\n",
    "consolidated_data = generate_consolidated_social_media_dataset(days=30)\n",
    "\n",
    "# Save to CSV\n",
    "consolidated_data.to_csv('synthetic_social_media_data_consolidated.csv', index=False)\n",
    "print(f\"Saved consolidated data with {len(consolidated_data)} sessions\")\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"\\nDataset shape: {consolidated_data.shape}\")\n",
    "print(f\"Sessions per day range: {consolidated_data.groupby('day').size().min()} - {consolidated_data.groupby('day').size().max()}\")\n",
    "print(f\"Average sessions per day: {consolidated_data.groupby('day').size().mean():.2f}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(consolidated_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9b7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
